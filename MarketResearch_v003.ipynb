{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarketResearch_v003.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true,
      "authorship_tag": "ABX9TyOQ+53t8ZpCxkzzesR8jgab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kappandrew2/DataPreProcessing/blob/main/MarketResearch_v003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mSu-6RRLTR1"
      },
      "source": [
        "#Model Purpose\n",
        "\n",
        "Utilize historical value and time attributes to predict the next day's gain or loss value\n",
        "\n",
        "!Dataset Notes: The dataset for this data solution must come from the following web sit and contain a large historical sample of data. For example:\n",
        "\n",
        "Begin Date = 12/01/2007 (Be mindful that the last 35 periods (in this case, days) will get chopped off of the bottom of the dataset during data preprocessing)\n",
        "\n",
        "End Date = Today's current value (to be run an hour before market close)\n",
        "\n",
        "Ticker = SPY\n",
        "\n",
        "Train Set = all data except last 60 periods (rows)\n",
        "\n",
        "Prediction Set = all data from -90 periods (days) to current\n",
        "\n",
        "https://www.wsj.com/market-data/quotes/index/SPX/historical-prices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx9d2QEa6mXf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import date, datetime, timedelta\n",
        "from pandas._libs.tslibs.timestamps import Timestamp\n",
        "from scipy.sparse import csr_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mLzKv5BGUUZ"
      },
      "source": [
        "#Connect to drive and import data set\n",
        "\n",
        "Using google drive\n",
        "\n",
        "Importing historical prices for ticker \"SPY\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAo4DboL6vQh",
        "outputId": "ad6a81fe-adba-4ea6-eecd-657a54f51152"
      },
      "source": [
        "#Create CSV from data export\n",
        "#https://www.wsj.com/market-data/quotes/index/SPX/historical-prices\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "dataset1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/HistoricalPricesSPY.csv')\n",
        "\n",
        "dataset2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/HistoricalPricesSH.csv')\n",
        "\n",
        "print(dataset1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "          Date    Open      High     Low     Close     Volume\n",
            "0     08/06/21  442.10  442.9400  441.80  442.4900   46930008\n",
            "1     08/05/21  440.22  441.8500  439.88  441.7600   38969660\n",
            "2     08/04/21  439.78  441.1243  438.73  438.9800   46732207\n",
            "3     08/03/21  438.44  441.2800  436.10  441.1500   58053898\n",
            "4     08/02/21  440.34  440.9300  437.21  437.5900   58783301\n",
            "...        ...     ...       ...     ...       ...        ...\n",
            "1404  01/08/16  195.19  195.8500  191.58  191.9230  209817203\n",
            "1405  01/07/16  195.33  197.4400  193.59  194.0500  213436094\n",
            "1406  01/06/16  198.34  200.0600  197.60  198.8200  152112609\n",
            "1407  01/05/16  201.40  201.9000  200.05  201.3600  110845797\n",
            "1408  01/04/16  200.49  201.0300  198.59  201.0192  222353500\n",
            "\n",
            "[1409 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWGjihLGGnuD"
      },
      "source": [
        "#Modifiy dataset Content and Headers\n",
        "\n",
        "Remove contents not required for this exercise\n",
        "\n",
        "Renaming columns to remove leading white space\n",
        "\n",
        "Narrowing the dataset can be done via drop or select, both options are available (comment out the one not in use)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_0Dq0yU7GHg"
      },
      "source": [
        "#Reduce dataframe columns\n",
        "dataset1.rename({' Close': 'Close'}, axis=1, inplace = True)\n",
        "dataset1 = dataset1[['Close', 'Date']]\n",
        "#Reduce Dataframe columns\n",
        "dataset2.rename({' Close': 'Close'}, axis=1, inplace = True)\n",
        "dataset2 = dataset2[['Close', 'Date']]\n",
        "\n",
        "#Set data types\n",
        "dataset1['Date'] = pd.to_datetime(dataset1['Date'])\n",
        "#Set data types\n",
        "dataset2['Date'] = pd.to_datetime(dataset2['Date'])\n",
        "\n",
        "#Create date attributes\n",
        "dataset1['DOW'] = dataset1['Date'].dt.dayofweek\n",
        "dataset1['Month'] = dataset1['Date'].dt.month\n",
        "dataset1['Quarter'] = dataset1['Date'].dt.quarter\n",
        "#Create date attributes\n",
        "dataset2['DOW'] = dataset2['Date'].dt.dayofweek\n",
        "dataset2['Month'] = dataset2['Date'].dt.month\n",
        "dataset2['Quarter'] = dataset2['Date'].dt.quarter\n",
        "\n",
        "#Date to sparse matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(drop='first')\n",
        "#Encode Time Variables\n",
        "dow1 = ohe.fit_transform(dataset1.DOW.values.reshape(-1,1)).toarray()\n",
        "DOW1 = pd.DataFrame(dow1, columns = ['Tu', 'We', 'Th', 'Fr'])\n",
        "month1 = ohe.fit_transform(dataset1.Month.values.reshape(-1,1)).toarray()\n",
        "MONTH1 = pd.DataFrame(month1, columns = ['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "q1 = ohe.fit_transform(dataset1.Quarter.values.reshape(-1,1)).toarray()\n",
        "Q1 = pd.DataFrame(q1, columns = ['Q2', 'Q3', 'Q4'])\n",
        "dataset1 = pd.concat([dataset1, DOW1, MONTH1, Q1], axis=1)\n",
        "#Encode Time Variables\n",
        "dow2 = ohe.fit_transform(dataset2.DOW.values.reshape(-1,1)).toarray()\n",
        "DOW2 = pd.DataFrame(dow2, columns = ['Tu', 'We', 'Th', 'Fr'])\n",
        "month2 = ohe.fit_transform(dataset2.Month.values.reshape(-1,1)).toarray()\n",
        "MONTH2 = pd.DataFrame(month2, columns = ['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "q2 = ohe.fit_transform(dataset2.Quarter.values.reshape(-1,1)).toarray()\n",
        "Q2 = pd.DataFrame(q2, columns = ['Q2', 'Q3', 'Q4'])\n",
        "dataset2 = pd.concat([dataset2, DOW2, MONTH2, Q2], axis=1)\n",
        "\n",
        "#Gain_loss_interval\n",
        "dataset1['gain_loss_0'] = dataset1['Close'].diff(-1)\n",
        "dataset1['gain_loss_1'] = dataset1['Close'].diff(-2)\n",
        "dataset1['gain_loss_2'] = dataset1['Close'].diff(-3)\n",
        "#Gain_loss_interval\n",
        "dataset2['gain_loss_0'] = dataset2['Close'].diff(-1)\n",
        "dataset2['gain_loss_1'] = dataset2['Close'].diff(-2)\n",
        "dataset2['gain_loss_2'] = dataset2['Close'].diff(-3)\n",
        "\n",
        "#Gain_loss_history\n",
        "dataset1['prior_day_1'] = dataset1['gain_loss_0']\n",
        "dataset1['prior_day_1'] = dataset1['prior_day_1'].shift(periods=-1, fill_value=0)\n",
        "dataset1['prior_day_2'] = dataset1['gain_loss_0']\n",
        "dataset1['prior_day_2'] = dataset1['prior_day_2'].shift(periods=-2, fill_value=0)\n",
        "#Gain_loss_history\n",
        "dataset2['prior_day_1'] = dataset2['gain_loss_0']\n",
        "dataset2['prior_day_1'] = dataset2['prior_day_1'].shift(periods=-1, fill_value=0)\n",
        "dataset2['prior_day_2'] = dataset2['gain_loss_0']\n",
        "dataset2['prior_day_2'] = dataset2['prior_day_2'].shift(periods=-2, fill_value=0)\n",
        "\n",
        "#Rolling means\n",
        "rolling_prior_day1 = dataset1['gain_loss_0']\n",
        "rolling_prior_day_5_1 = rolling_prior_day1[::-1].rolling(5).mean()[::-1]\n",
        "rolling_prior_day_10_1 = rolling_prior_day1[::-1].rolling(10).mean()[::-1]\n",
        "rolling_prior_day_5_1.rename('rolling_prior_day_5', inplace = True)\n",
        "rolling_prior_day_10_1.rename('rolling_prior_day_10', inplace = True)\n",
        "dataset = pd.concat([dataset1, rolling_prior_day_10_1, rolling_prior_day_10_1], axis = 1)\n",
        "#Rolling means\n",
        "rolling_prior_day2 = dataset2['gain_loss_0']\n",
        "rolling_prior_day_5_2 = rolling_prior_day2[::-1].rolling(5).mean()[::-1]\n",
        "rolling_prior_day_10_2 = rolling_prior_day2[::-1].rolling(10).mean()[::-1]\n",
        "rolling_prior_day_5_2.rename('rolling_prior_day_5', inplace = True)\n",
        "rolling_prior_day_10_2.rename('rolling_prior_day_10', inplace = True)\n",
        "dataset = pd.concat([dataset2, rolling_prior_day_10_2, rolling_prior_day_10_2], axis = 1)\n",
        "\n",
        "#Remove NaN rows\n",
        "dataset1.dropna(inplace = True)\n",
        "#Remove NaN rows\n",
        "dataset2.dropna(inplace = True)\n",
        "\n",
        "#add dependant variable (requires shifted down one row)\n",
        "y1 = pd.DataFrame(dataset1['gain_loss_0']).reset_index(drop = True)\n",
        "y1.loc[-1] = [0]\n",
        "y1.index = y1.index + 1\n",
        "y1 = y1.sort_index()\n",
        "y1.drop(y1.tail(1).index, inplace = True)\n",
        "y1.rename(columns={'gain_loss_0': 'y'}, inplace=True)\n",
        "y_df1 = pd.DataFrame(y1, columns=['y'])\n",
        "dataset_final1 = pd.concat([dataset1, y_df1], axis=1)\n",
        "#add dependant variable (requires shifted down one row)\n",
        "y2 = pd.DataFrame(dataset2['gain_loss_0']).reset_index(drop = True)\n",
        "y2.loc[-1] = [0]\n",
        "y2.index = y2.index + 1\n",
        "y2 = y2.sort_index()\n",
        "y2.drop(y2.tail(1).index, inplace = True)\n",
        "y2.rename(columns={'gain_loss_0': 'y'}, inplace=True)\n",
        "y_df2 = pd.DataFrame(y2, columns=['y'])\n",
        "dataset_final2 = pd.concat([dataset2, y_df2], axis=1)\n",
        "\n",
        "#drop NaN on final datase\n",
        "dataset_final1.dropna(inplace = True)\n",
        "#drop NaN on final datase\n",
        "dataset_final2.dropna(inplace = True)\n",
        "\n",
        "#add prefix to all columns\n",
        "dataset_final1.columns = [str(col) + '_1' for col in dataset_final1.columns]\n",
        "#add prefix to all columns\n",
        "dataset_final2.columns = [str(col) + '_2' for col in dataset_final2.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE9kCh_CNjtW"
      },
      "source": [
        "#Concatenate the dataset\n",
        "dataset_final = pd.concat([dataset_final1, dataset_final2], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdl7ZzP86Sl6"
      },
      "source": [
        "#Determine Attributes to Keep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63qL6jw_TOvu"
      },
      "source": [
        "#identify columns for data model\n",
        "selection_value_upper = .06\n",
        "corr_mxa = dataset_final.corr()\n",
        "corr_mxb = corr_mxa[corr_mxa['y_1'].abs()>selection_value]\n",
        "corr_mxb = corr_mxb.T.columns.values\n",
        "add_back = 'Date_1'\n",
        "corr_mxb = np.concatenate((corr_mxb, add_back), axis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDwF5lNfVDWY",
        "outputId": "9f41bc29-31b6-44a3-efcb-7a39e800ddd5"
      },
      "source": [
        "#add needed attributes back into the model\n",
        "dataset_final = dataset_final[corr_mxb]\n",
        "dataset_final['Date_1'] = pd.to_datetime(dataset_final['Date_1'])\n",
        "dataset_final['Close_1'] = dataset_final1['Close_1']\n",
        "dataset_final['Close_2'] = dataset_final2['Close_2']\n",
        "print(dataset_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      DOW_1  Quarter_1  We_1  Mar_1  ...  prior_day_2_2   y_2     Date_1   Close_1\n",
            "0         4          3   0.0    0.0  ...           0.07  0.00 2021-08-06  442.4900\n",
            "1         3          3   0.0    0.0  ...          -0.13 -0.02 2021-08-05  441.7600\n",
            "2         2          3   1.0    0.0  ...           0.04 -0.10 2021-08-04  438.9800\n",
            "3         1          3   0.0    0.0  ...           0.07  0.07 2021-08-03  441.1500\n",
            "4         0          3   0.0    0.0  ...          -0.07 -0.13 2021-08-02  437.5900\n",
            "...     ...        ...   ...    ...  ...            ...   ...        ...       ...\n",
            "1401      2          1   1.0    0.0  ...           0.00 -0.78 2016-01-13  188.8300\n",
            "1402      1          1   0.0    0.0  ...           0.48  1.18 2016-01-12  193.6608\n",
            "1403      0          1   0.0    0.0  ...           0.98 -0.44 2016-01-11  192.1100\n",
            "1404      4          1   0.0    0.0  ...           0.60  0.00 2016-01-08  191.9230\n",
            "1405      3          1   0.0    0.0  ...          -0.12  0.48 2016-01-07  194.0500\n",
            "\n",
            "[1406 rows x 38 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rifSXtgbhYYt"
      },
      "source": [
        "#Create dataset splitting variable (Train and Pred)\n",
        "\n",
        "Date variables based on today date are required to prevent \"hardcoding\" dates into the model\n",
        "\n",
        "The date_var variable will represent the most current date in the dataset. This allows the solution to be run for any timeframe.\n",
        "\n",
        "The following code can replace the current date_var logic in the case the current method causes issues. Note, this method requires adjustment when back testing \n",
        "\n",
        "date_var = pd.to_datetime(date.today()) \n",
        "\n",
        "use train/pred_minus_days to tune the model for longer or shorter periods of time. Allow for a 30 day overlap where pred will have 30 days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF1y7B2r7cVv",
        "outputId": "6683de2e-d0bf-4f4e-c8dc-5b8633796f3b"
      },
      "source": [
        "train_minus_days = 180\n",
        "pred_minus_days = 150\n",
        "\n",
        "date_var = dataset_final['Date_1'].max()\n",
        "train_begin_date = dataset_final['Date_1'].min()\n",
        "train_end_date = (date_var - pd.to_timedelta(train_minus_days, unit='d'))\n",
        "pred_begin_date = (date_var - pd.to_timedelta(pred_minus_days, unit='d'))\n",
        "pred_end_date = date_var\n",
        "\n",
        "train_begin_date = train_begin_date.to_pydatetime()\n",
        "train_end_date = train_end_date.to_pydatetime()\n",
        "pred_begin_date = pred_begin_date.to_pydatetime()\n",
        "pred_end_date = pred_end_date.to_pydatetime()\n",
        "\n",
        "\n",
        "print(train_begin_date)\n",
        "print(train_end_date)\n",
        "print(pred_begin_date)\n",
        "print(pred_end_date)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2016-01-07 00:00:00\n",
            "2021-02-07 00:00:00\n",
            "2021-03-09 00:00:00\n",
            "2021-08-06 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzgziXsl-SBz"
      },
      "source": [
        "#Split between training and predict data sets\n",
        "\n",
        "The top last 90 periods (rows) will generate the pred data set.\n",
        "\n",
        "All but the top 60 periods (rows) will generate the training data set.\n",
        "\n",
        "The 30 day overlap can provide a measure of the model's degredation over time\n",
        "\n",
        "!Note - The market is closed on weekends and holidays. The count of days in each set will NOT equal the amount of days between begin and end dates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOHf760CShq3",
        "outputId": "eb3be482-b38a-4424-c348-0855ae324dba"
      },
      "source": [
        "#split text and train datasets\n",
        "predset = dataset_final[(dataset_final['Date_1'] >= pred_begin_date) & \n",
        "                        (dataset_final['Date_1'] <= pred_end_date)]\n",
        "trainset = dataset_final[(dataset_final['Date_1'] >= train_begin_date) & \n",
        "                         (dataset_final['Date_1'] <= train_end_date)]\n",
        "type(predset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 733
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTZ65w52bsBZ",
        "outputId": "c4fff0a8-9552-42a6-d678-c4b4038f9fc4"
      },
      "source": [
        "list(dataset_final.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DOW_1',\n",
              " 'Quarter_1',\n",
              " 'We_1',\n",
              " 'Mar_1',\n",
              " 'Apr_1',\n",
              " 'Jul_1',\n",
              " 'Aug_1',\n",
              " 'Sep_1',\n",
              " 'Oct_1',\n",
              " 'Nov_1',\n",
              " 'Dec_1',\n",
              " 'Q2_1',\n",
              " 'gain_loss_0_1',\n",
              " 'gain_loss_1_1',\n",
              " 'gain_loss_2_1',\n",
              " 'prior_day_1_1',\n",
              " 'y_1',\n",
              " 'Close_2',\n",
              " 'DOW_2',\n",
              " 'Quarter_2',\n",
              " 'We_2',\n",
              " 'Mar_2',\n",
              " 'Apr_2',\n",
              " 'Jul_2',\n",
              " 'Aug_2',\n",
              " 'Sep_2',\n",
              " 'Oct_2',\n",
              " 'Nov_2',\n",
              " 'Dec_2',\n",
              " 'Q2_2',\n",
              " 'gain_loss_0_2',\n",
              " 'gain_loss_1_2',\n",
              " 'gain_loss_2_2',\n",
              " 'prior_day_1_2',\n",
              " 'prior_day_2_2',\n",
              " 'y_2',\n",
              " 'Date_1',\n",
              " 'Close_1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 734
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50h5EJr9_DZW"
      },
      "source": [
        "#Convert dataset into X and y and refine column membership\n",
        "\n",
        "This process separates the dependant and independant variables\n",
        "\n",
        "X should not contain the y or yb attributes\n",
        "\n",
        "for X, \"Date\" should be removed since it is a time-series value; date attributes will represent time\n",
        "\n",
        "for X, \"Close\" should be removed due to its relationship to the indepenant variable\n",
        "\n",
        "Two models will come out of this model, one for continuous variable y and binary value yb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0cZJQK3ILwd",
        "outputId": "0037c76c-8967-49a3-9a27-ebca15d87097"
      },
      "source": [
        "XX = trainset\n",
        "XX.drop(XX.tail(31).index, inplace = True)\n",
        "X = XX.drop(['y_1', 'y_2', 'Date_1', 'Close_1', 'Close_2'], axis=1)\n",
        "y_1 = trainset['y_1'].values\n",
        "y_2 = trainset['y_2'].values\n",
        "print(X)\n",
        "print(\"-------------------------\")\n",
        "print(y_1)\n",
        "print(\"-------------------------\")\n",
        "print(y_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      DOW_1  Quarter_1  We_1  ...  gain_loss_2_2  prior_day_1_2  prior_day_2_2\n",
            "126       4          1   0.0  ...          -0.30          -0.20          -0.02\n",
            "127       3          1   0.0  ...          -0.47          -0.02          -0.25\n",
            "128       2          1   1.0  ...          -0.55          -0.25          -0.28\n",
            "129       1          1   0.0  ...          -0.19          -0.28           0.34\n",
            "130       0          1   0.0  ...          -0.08           0.34          -0.14\n",
            "...     ...        ...   ...  ...            ...            ...            ...\n",
            "1370      0          1   0.0  ...          -0.06           0.10          -0.56\n",
            "1371      4          1   0.0  ...          -0.68          -0.56          -0.22\n",
            "1372      3          1   0.0  ...          -0.18          -0.22           0.60\n",
            "1373      2          1   1.0  ...          -0.34           0.60          -0.72\n",
            "1374      1          1   0.0  ...          -0.06          -0.72           0.06\n",
            "\n",
            "[1249 rows x 33 columns]\n",
            "-------------------------\n",
            "[ 2.8     1.52    4.34   ... -0.4508  2.34    0.88  ]\n",
            "-------------------------\n",
            "[-0.11 -0.08 -0.2  ...  0.1  -0.56 -0.22]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQNhBiAEN_fx",
        "outputId": "0f706716-48a2-48da-d341-9c476f8625ba"
      },
      "source": [
        "#Validation of row counts\n",
        "\n",
        "f = len(X)\n",
        "g = len(y_1)\n",
        "h = len(y_2)\n",
        "\n",
        "print(f, g, h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1249 1249 1249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGLmBg2CCjH"
      },
      "source": [
        "#Train the models\n",
        "\n",
        "The model can be extended to use any regression or classificaiton model.\n",
        "\n",
        "Current model inventory:\n",
        "\n",
        "1) Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4xd8XMCH3M2",
        "outputId": "e9afe75a-6e68-4f15-940f-6824fc6bc648"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "regressor = RandomForestRegressor(n_estimators = 10, random_state = 2)\n",
        "\n",
        "regressor.fit(X, y_1)\n",
        "\n",
        "regressor_b = RandomForestRegressor(n_estimators = 10, random_state = 1)\n",
        "\n",
        "regressor_b.fit(X, y_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
              "                      random_state=1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 737
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMP2XIlOCUTS"
      },
      "source": [
        "#Create predict dataset\n",
        "\n",
        "The predict dataset should match the process used to generate the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0BNR3t8Um8v",
        "outputId": "0b8f6050-d0ad-4b02-becb-95a8dfa202a1"
      },
      "source": [
        "#Prepare predict set\n",
        "Xpred = predset\n",
        "X_pred = Xpred.drop(['y_1', 'y_2', 'Date_1', 'Close_1', 'Close_2'], axis=1)\n",
        "y_actual = predset['y_1'].values\n",
        "yb_actual = predset['y_2'].values\n",
        "print(X_pred)\n",
        "print(\"-------------------------\")\n",
        "print(y_actual)\n",
        "print(\"-------------------------\")\n",
        "print(yb_actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     DOW_1  Quarter_1  We_1  ...  gain_loss_2_2  prior_day_1_2  prior_day_2_2\n",
            "0        4          3   0.0  ...          -0.05          -0.10           0.07\n",
            "1        3          3   0.0  ...          -0.16           0.07          -0.13\n",
            "2        2          3   1.0  ...          -0.02          -0.13           0.04\n",
            "3        1          3   0.0  ...          -0.02           0.04           0.07\n",
            "4        0          3   0.0  ...           0.04           0.07          -0.07\n",
            "..     ...        ...   ...  ...            ...            ...            ...\n",
            "101      0          1   0.0  ...          -0.32          -0.04          -0.18\n",
            "102      4          1   0.0  ...          -0.31          -0.18          -0.09\n",
            "103      3          1   0.0  ...          -0.51          -0.09          -0.24\n",
            "104      2          1   1.0  ...          -0.25          -0.24           0.08\n",
            "105      1          1   0.0  ...          -0.49           0.08          -0.33\n",
            "\n",
            "[106 rows x 33 columns]\n",
            "-------------------------\n",
            "[ 0.    0.73  2.78 -2.17  3.56 -0.92 -2.14  1.82 -0.18 -2.01  1.08  4.48\n",
            "  0.91  3.49  6.09 -6.37 -3.41 -1.49  0.65 -1.49  1.56  4.6  -3.54  1.53\n",
            " -0.79  3.29  2.37  0.36  0.23  0.86  1.51  2.5  -0.51  2.25  5.94 -7.05\n",
            " -0.14 -2.37 -0.78  0.95  0.7   1.96 -0.63  0.09 -0.41  3.83 -1.56  0.66\n",
            " -0.37  0.75  0.22  0.83 -0.93  4.23 -0.34  4.42 -1.08 -3.58 -1.06  6.3\n",
            "  4.87 -8.8  -3.73 -4.18  3.05  3.32  0.13 -2.58  0.9  -2.76  2.66 -0.12\n",
            " -0.09  0.87  4.47 -3.8   3.9  -3.04 -2.05  1.39  4.42 -1.41  1.22  0.15\n",
            "  2.97  1.93  0.47 -0.24  5.75  4.28  1.6  -1.05 -0.2   6.28  2.18 -1.98\n",
            " -3.09  3.11 -2.   -5.78  1.35 -0.5   2.35  0.53  3.95  2.41]\n",
            "-------------------------\n",
            "[ 0.   -0.02 -0.1   0.07 -0.13  0.04  0.07 -0.07  0.02  0.06 -0.04 -0.16\n",
            " -0.03 -0.11 -0.24  0.23  0.12  0.05 -0.02  0.06 -0.06 -0.16  0.11 -0.06\n",
            "  0.05 -0.13 -0.08 -0.01 -0.01 -0.03 -0.06 -0.09  0.01 -0.08 -0.22  0.21\n",
            " -0.01  0.09  0.03 -0.04 -0.02 -0.08  0.02  0.01  0.01 -0.14  0.06 -0.04\n",
            "  0.01 -0.03 -0.01 -0.02  0.03 -0.17  0.02 -0.17  0.03  0.15  0.05 -0.27\n",
            " -0.2   0.36  0.13  0.16 -0.12 -0.13 -0.01  0.11 -0.05  0.11 -0.11  0.01\n",
            "  0.02 -0.05 -0.17  0.14 -0.15  0.13  0.06 -0.05 -0.16  0.05 -0.05 -0.01\n",
            " -0.13 -0.07 -0.01  0.   -0.24 -0.17 -0.08  0.05  0.   -0.26 -0.1   0.08\n",
            "  0.14 -0.15  0.04  0.23 -0.04  0.02 -0.1  -0.04 -0.18 -0.09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2GHt6_yClWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9fbfa4-7a9e-4b88-f64a-d078be02f01f"
      },
      "source": [
        "i = len(Xpred)\n",
        "j = len(y_actual)\n",
        "k = len(yb_actual)\n",
        "\n",
        "print(i, j, k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "106 106 106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agrPLswhEQYf"
      },
      "source": [
        "#Generate predictions\n",
        "\n",
        "Predictions are made for both continuous and binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJYXf1mKU9t2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad203c9e-432f-4952-8afb-378a5a9d5b69"
      },
      "source": [
        "y_1_pred = regressor.predict(X_pred)\n",
        "\n",
        "y_2_pred = regressor_b.predict(X_pred)\n",
        "\n",
        "#y_all_pred = regressor_all.predict(X_all_pred)\n",
        "\n",
        "#y_all_pred_b = regressor_all_b.predict(X_all_pred)\n",
        "\n",
        "np_array = np.concatenate((y_1_pred.reshape(len(y_1_pred),1), \n",
        "                           y_2_pred.reshape(len(y_2_pred),1),\n",
        "                           y_actual.reshape(len(y_actual),1),\n",
        "                           yb_actual.reshape(len(yb_actual),1),\n",
        "                           ), axis = 1)\n",
        "\n",
        "results = pd.DataFrame(np_array, columns = ['y_1_pred', 'y_2_pred', 'y_1_actual', 'y_2_actual'])\n",
        "\n",
        "#########################need to shift y values up by one\n",
        "\n",
        "\n",
        "sp500_pred = results.head(5)\n",
        "\n",
        "print(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     y_1_pred  y_2_pred  y_1_actual  y_2_actual\n",
            "0    -0.17100    -0.074        0.00        0.00\n",
            "1     0.24000    -0.067        0.73       -0.02\n",
            "2     1.56900    -0.010        2.78       -0.10\n",
            "3    -0.87600     0.041       -2.17        0.07\n",
            "4     1.14100    -0.095        3.56       -0.13\n",
            "..        ...       ...         ...         ...\n",
            "101  -1.63000     0.060       -0.50        0.02\n",
            "102   1.64700    -0.049        2.35       -0.10\n",
            "103   0.93100    -0.140        0.53       -0.04\n",
            "104  -0.10808    -0.043        3.95       -0.18\n",
            "105  -0.80800     0.031        2.41       -0.09\n",
            "\n",
            "[106 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-3X7j0LFdm3"
      },
      "source": [
        "#Create buy/sell indicator/strategy based on pred\n",
        "\n",
        "0 = Buy nothing\n",
        "\n",
        "1 = Buy market\n",
        "\n",
        "2 = Buy market short\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGmNBoUvD3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ffbb99b1-bf0b-4935-e749-eb4b67cf0c36"
      },
      "source": [
        "results['y_gain_pred_arg'] = np.where((np.where(results['y_1_pred'] > 0, 1, 0) + np.where(results['y_2_pred'] > 0, 1, 0)) > 0, 1, 0)\n",
        "results['purchase'] = np.where(results['y_gain_pred_arg'] > 0, np.where(results['y_1_pred'] > results['y_2_pred'], 1, 2), 0)\n",
        "results['Close_1'] = Xpred['Close_1']\n",
        "results['Close_2'] = Xpred['Close_2']\n",
        "results['market'] = results['y_1_actual']/results['Close_1']\n",
        "results['purchase_gain_loss'] = np.where(results['purchase'] == 1, #Change these between 1, 2 and 0 to alter the model behavior\n",
        "                                         results['y_1_actual']/results['Close_1'], \n",
        "                                         np.where(results['purchase'] == 2, #Change these between 1, 2 and 0 to alter the model behavior\n",
        "                                                  results['y_2_actual']/results['Close_2'], 0)) #Change these between 1, 2 and 0 to alter the model behavior\n",
        "results['market_cumu'] = results.market[::-1].cumsum()\n",
        "results['purchase_gain_loss_cumu'] = results.purchase_gain_loss[::-1].cumsum()\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_1_pred</th>\n",
              "      <th>y_2_pred</th>\n",
              "      <th>y_1_actual</th>\n",
              "      <th>y_2_actual</th>\n",
              "      <th>y_gain_pred_arg</th>\n",
              "      <th>purchase</th>\n",
              "      <th>Close_1</th>\n",
              "      <th>Close_2</th>\n",
              "      <th>market</th>\n",
              "      <th>purchase_gain_loss</th>\n",
              "      <th>market_cumu</th>\n",
              "      <th>purchase_gain_loss_cumu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.17100</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>442.49</td>\n",
              "      <td>14.86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136208</td>\n",
              "      <td>0.084649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.24000</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>441.76</td>\n",
              "      <td>14.88</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.136208</td>\n",
              "      <td>0.084649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.56900</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>2.78</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>438.98</td>\n",
              "      <td>14.98</td>\n",
              "      <td>0.006333</td>\n",
              "      <td>0.006333</td>\n",
              "      <td>0.134556</td>\n",
              "      <td>0.082997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.87600</td>\n",
              "      <td>0.041</td>\n",
              "      <td>-2.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>441.15</td>\n",
              "      <td>14.91</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>0.004695</td>\n",
              "      <td>0.128223</td>\n",
              "      <td>0.076664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.14100</td>\n",
              "      <td>-0.095</td>\n",
              "      <td>3.56</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>437.59</td>\n",
              "      <td>15.04</td>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.133142</td>\n",
              "      <td>0.071969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-1.63000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>396.41</td>\n",
              "      <td>16.83</td>\n",
              "      <td>-0.001261</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.022413</td>\n",
              "      <td>0.003278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1.64700</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>2.35</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>394.06</td>\n",
              "      <td>16.93</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.023674</td>\n",
              "      <td>0.002090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.93100</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>0.53</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>393.53</td>\n",
              "      <td>16.97</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.017711</td>\n",
              "      <td>-0.003874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-0.10808</td>\n",
              "      <td>-0.043</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>389.58</td>\n",
              "      <td>17.15</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016364</td>\n",
              "      <td>-0.005220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-0.80800</td>\n",
              "      <td>0.031</td>\n",
              "      <td>2.41</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>387.17</td>\n",
              "      <td>17.24</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>-0.005220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     y_1_pred  y_2_pred  ...  market_cumu  purchase_gain_loss_cumu\n",
              "0    -0.17100    -0.074  ...     0.136208                 0.084649\n",
              "1     0.24000    -0.067  ...     0.136208                 0.084649\n",
              "2     1.56900    -0.010  ...     0.134556                 0.082997\n",
              "3    -0.87600     0.041  ...     0.128223                 0.076664\n",
              "4     1.14100    -0.095  ...     0.133142                 0.071969\n",
              "..        ...       ...  ...          ...                      ...\n",
              "101  -1.63000     0.060  ...     0.022413                 0.003278\n",
              "102   1.64700    -0.049  ...     0.023674                 0.002090\n",
              "103   0.93100    -0.140  ...     0.017711                -0.003874\n",
              "104  -0.10808    -0.043  ...     0.016364                -0.005220\n",
              "105  -0.80800     0.031  ...     0.006225                -0.005220\n",
              "\n",
              "[106 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 741
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1sxZIZRJrCl"
      },
      "source": [
        "results['market_cumu'] = results.market[::-1].cumsum()\n",
        "results['purchase_gain_loss_cumu'] = results.purchase_gain_loss[::-1].cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "EzunEjyZ3BtJ",
        "outputId": "45214557-b12d-4d60-bcd2-e5b2db803556"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_1_pred</th>\n",
              "      <th>y_2_pred</th>\n",
              "      <th>y_1_actual</th>\n",
              "      <th>y_2_actual</th>\n",
              "      <th>y_gain_pred_arg</th>\n",
              "      <th>purchase</th>\n",
              "      <th>Close_1</th>\n",
              "      <th>Close_2</th>\n",
              "      <th>market</th>\n",
              "      <th>purchase_gain_loss</th>\n",
              "      <th>market_cumu</th>\n",
              "      <th>purchase_gain_loss_cumu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.17100</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>442.49</td>\n",
              "      <td>14.86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136208</td>\n",
              "      <td>0.084649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.24000</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>441.76</td>\n",
              "      <td>14.88</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.136208</td>\n",
              "      <td>0.084649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.56900</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>2.78</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>438.98</td>\n",
              "      <td>14.98</td>\n",
              "      <td>0.006333</td>\n",
              "      <td>0.006333</td>\n",
              "      <td>0.134556</td>\n",
              "      <td>0.082997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.87600</td>\n",
              "      <td>0.041</td>\n",
              "      <td>-2.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>441.15</td>\n",
              "      <td>14.91</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>0.004695</td>\n",
              "      <td>0.128223</td>\n",
              "      <td>0.076664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.14100</td>\n",
              "      <td>-0.095</td>\n",
              "      <td>3.56</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>437.59</td>\n",
              "      <td>15.04</td>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.133142</td>\n",
              "      <td>0.071969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-1.63000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>396.41</td>\n",
              "      <td>16.83</td>\n",
              "      <td>-0.001261</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.022413</td>\n",
              "      <td>0.003278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1.64700</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>2.35</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>394.06</td>\n",
              "      <td>16.93</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.023674</td>\n",
              "      <td>0.002090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.93100</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>0.53</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>393.53</td>\n",
              "      <td>16.97</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.017711</td>\n",
              "      <td>-0.003874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-0.10808</td>\n",
              "      <td>-0.043</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>389.58</td>\n",
              "      <td>17.15</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016364</td>\n",
              "      <td>-0.005220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-0.80800</td>\n",
              "      <td>0.031</td>\n",
              "      <td>2.41</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>387.17</td>\n",
              "      <td>17.24</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>-0.005220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     y_1_pred  y_2_pred  ...  market_cumu  purchase_gain_loss_cumu\n",
              "0    -0.17100    -0.074  ...     0.136208                 0.084649\n",
              "1     0.24000    -0.067  ...     0.136208                 0.084649\n",
              "2     1.56900    -0.010  ...     0.134556                 0.082997\n",
              "3    -0.87600     0.041  ...     0.128223                 0.076664\n",
              "4     1.14100    -0.095  ...     0.133142                 0.071969\n",
              "..        ...       ...  ...          ...                      ...\n",
              "101  -1.63000     0.060  ...     0.022413                 0.003278\n",
              "102   1.64700    -0.049  ...     0.023674                 0.002090\n",
              "103   0.93100    -0.140  ...     0.017711                -0.003874\n",
              "104  -0.10808    -0.043  ...     0.016364                -0.005220\n",
              "105  -0.80800     0.031  ...     0.006225                -0.005220\n",
              "\n",
              "[106 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 743
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5teAplemSXTI"
      },
      "source": [
        "The following section should be turned into a loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptcaYXPj-FvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fce6e8bb-a844-4249-9882-9774bfca2893"
      },
      "source": [
        "plt.plot(results['market_cumu'], color='black')\n",
        "plt.plot(results['purchase_gain_loss_cumu'], color='red')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd52059cbd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 744
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1HSPFzYmaf"
      },
      "source": [
        "#results.to_csv('/content/drive/MyDrive/Colab Notebooks/results_sky_hibs_20160101-20210803_.csv')\n",
        "#performance.to_csv('/content/drive/MyDrive/Colab Notebooks/performance_sky_hibs_20190101-20210803_.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}